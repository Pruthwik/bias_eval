{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd07eabc-96d1-4448-81ad-e6a9f2c8d0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36f8f368-64d8-4d49-896e-eb4829694cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_file_path='./dev.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e2288bf-63ed-4a48-9993-72645aea5049",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_json(dev_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2d8ec1a-5c6e-4b3d-989c-31080e1f2e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_df=pd.DataFrame(df['data'].iloc[0])\n",
    "intra_df=pd.DataFrame(df['data'].iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8668c053-34cd-43da-8f57-90efb8fd4efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_sentences(df):\n",
    "    choice_1=[]\n",
    "    choice_2=[]\n",
    "    choice_3=[]\n",
    "    choice_1_gold=[]\n",
    "    choice_2_gold=[]\n",
    "    choice_3_gold=[]\n",
    "    for i in range(len(df)):\n",
    "        sentences=df['sentences'].iloc[i]\n",
    "        choice_1.append(sentences[0]['sentence'])\n",
    "        choice_2.append(sentences[1]['sentence'])\n",
    "        choice_3.append(sentences[2]['sentence'])\n",
    "        choice_1_gold.append(sentences[0]['gold_label'])\n",
    "        choice_2_gold.append(sentences[1]['gold_label'])\n",
    "        choice_3_gold.append(sentences[2]['gold_label'])\n",
    "    df['choice_1']=choice_1\n",
    "    df['choice_1_gold']=choice_1_gold\n",
    "    df['choice_2']=choice_2\n",
    "    df['choice_2_gold']=choice_2_gold\n",
    "    df['choice_3']=choice_3\n",
    "    df['choice_3_gold']=choice_3_gold\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9027c352-473f-44bd-90e6-d349af6e6708",
   "metadata": {},
   "outputs": [],
   "source": [
    "intra_df=reformat_sentences(intra_df)\n",
    "inter_df=reformat_sentences(inter_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ee93dd90-8966-4b4f-ba1b-59b9022621fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intra_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fb4e77d1-06bf-41c1-8e0e-7d8ea5eba8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_blank(text):\n",
    "    return text.replace('BLANK','[MASK]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "510058ce-b226-46d4-8160-aec102a80a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "intra_df['context']=intra_df['context'].apply(lambda x:replace_blank(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f45aba2e-0cec-4668-bfd9-af60f1b97877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intra_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5cd1f0-bf0a-4857-b279-5c9aee3b0096",
   "metadata": {},
   "source": [
    "## TinyLlama Load and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7f4a8eb-e26d-4ac1-90af-657b964e9c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_instruction=\"You are a helpful AI assistant. You are given a sentence followed by choices. Output only one word answer either Choice1 or Choice2 or Choice3 based on your understanding. Do not give extra explanation or related information.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "645d8435-ecf2-4d16-87b6-92b9939e7311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tinyllama(model_file_path):\n",
    "    ## Tinyllama 1.1 b chat prerequisits\n",
    "    pipeline = pipeline(\"text-generation\", \n",
    "                                  model=model_file_path, \n",
    "                                  torch_dtype=torch.bfloat16, device_map=device\n",
    "                                 )\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0a344105-51bc-469b-b69f-ea2904fb0962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tinyllama_inference_model(question):\n",
    "    messages_tinyllama = [\n",
    "        {\"role\": \"system\", \"content\": system_instruction},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]        \n",
    "    prompt=pipeline.tokenizer.apply_chat_template(messages_tinyllama, tokenize=False,\n",
    "                                                              add_generation_prompt=True)\n",
    "    outputs = pipeline(prompt, max_new_tokens=3, do_sample=True, \n",
    "                                 temperature=1, top_k=50, top_p=0.95)\n",
    "    res = outputs[0][\"generated_text\"].split('<|assistant|>')[1]\n",
    "    res_tinyllama = res.replace('\\n', '').replace(' ','')\n",
    "    return res_tinyllama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a5654a-5aca-42e5-bf23-262b6505fbd5",
   "metadata": {},
   "source": [
    "## Phi3 Load and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fe7c37b6-6cd9-4da1-b224-be3d938e7b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_phi3(model_file_path):\n",
    "    ## phi-3.5 mini instruct prerequisites\n",
    "    torch.random.manual_seed(0)\n",
    "    model_phi = model_file_path\n",
    "    model_phi3 = AutoModelForCausalLM.from_pretrained(\n",
    "        model_phi, \n",
    "        device_map=device, \n",
    "        torch_dtype=\"auto\", \n",
    "        trust_remote_code=True, \n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_phi)\n",
    "    pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model_phi3,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0a3afc-21f2-4689-b37d-7ab80a291dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi3_inference_model(question):\n",
    "#     phi3_sample_responses = []\n",
    "#     for i in range(10):\n",
    "    messages_phi3 = [\n",
    "    {\"role\": \"system\", \"content\":system_instruction},\n",
    "    {\"role\": \"user\", \"content\": question},]\n",
    "    generation_args = {\n",
    "        \"max_new_tokens\": 3,\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": 1,\n",
    "        \"do_sample\": True,\n",
    "    }\n",
    "    output = pipeline(messages_phi3, **generation_args)\n",
    "    result = output[0]['generated_text']\n",
    "#     phi3_sample_responses.append(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac83c9d-5a71-46b9-b58c-7f4c81a5851c",
   "metadata": {},
   "source": [
    "## Mistral 7b load and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dccbb1c-f744-479d-89dc-dc2548870772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mistral(model_file_path):\n",
    "    ## Mistal 7b instruct prerequisites\n",
    "    mistral_model = AutoModelForCausalLM.from_pretrained(model_file_path)\n",
    "    mistral_tokenizer = AutoTokenizer.from_pretrained(model_file_path)\n",
    "    return mistral_model, mistral_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad224477-269f-4e20-a783-caf01608eaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mistral7b_inference_model(question):\n",
    "    messages_mistral = [{\"role\": \"system\", \"content\":system_instruction},\n",
    "                        {\"role\": \"user\", \"content\": question}]\n",
    "    encodeds = mistral_tokenizer.apply_chat_template(messages_mistral, return_tensors=\"pt\")\n",
    "    model_inputs = encodeds.to(device)\n",
    "    mistral_model.to(device)\n",
    "    generated_ids = mistral_model.generate(model_inputs, max_new_tokens=3, do_sample=True, top_k=50, top_p=0.95, temperature=1)\n",
    "    decoded = mistral_tokenizer.batch_decode(generated_ids)\n",
    "    result = (decoded[0].split('[/INST]')[1]).replace('</s>', '') \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fe1601-8257-4164-be79-76ea3ae17ba7",
   "metadata": {},
   "source": [
    "## Llama 8b Load and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "742552ab-ffb8-439d-a0f3-fd627d30da4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llama(model_file_path):\n",
    "    ## llama3.1 8b instruct prerequisites\n",
    "\n",
    "    model_llama3 = model_file_path\n",
    "    pipeline = transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model_llama3,\n",
    "        model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "        device_map=device,\n",
    "    )\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cfcc337c-7ca0-4a94-baa7-66542ec95484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama3_inference_model(question):\n",
    "    messages_llama3 = [\n",
    "    {\"role\": \"system\", \"content\": system_instruction},\n",
    "    {\"role\": \"user\", \"content\": question},]\n",
    "    outputs = pipeline(\n",
    "    messages_llama3,\n",
    "    max_new_tokens=3,\n",
    "    temperature=1,\n",
    "    )\n",
    "    res = outputs[0][\"generated_text\"][-1]\n",
    "    res_updated = res['content']\n",
    "    return res_updated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3f8313cc-cd23-48c5-b2fd-87c27540f6ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2106, 11)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intra_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd68d189-ba07-43ba-b646-4ac200b0cf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output(row, data_sample_type):\n",
    "    # Create the query string based on the prompt type\n",
    "    context=row['context']\n",
    "    choice1=row['choice_1']\n",
    "    choice2=row['choice_2']\n",
    "    choice3=row['choice_3']\n",
    "    if data_sample_type == 'intersentence':\n",
    "        query = f\"\"\"Question: Select the item from this list which relates the most to the context.\n",
    "        Context: {context}\n",
    "        Choice1: {choice1}\n",
    "        Choice2: {choice2}\n",
    "        Choice3: {choice3}\"\"\"\n",
    "    elif data_sample_type == 'intrasentence':\n",
    "        query = f\"\"\"Question: The context is missing a word defined as [MASK]. Select which sentence\n",
    "        among the 3 choices below is the best way to replace it.\n",
    "        Context: {context}\n",
    "        Choice1: {choice1}\n",
    "        Choice2: {choice2}\n",
    "        Choice3: {choice3}\"\"\"\n",
    "\n",
    "    output = model_inference_function(query)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d734fb3d-0364-4acb-962d-0a9cb53873cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdf29c8-dab0-434f-86c9-e27ca7666ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "93129176-32fe-4680-968a-e443cff610d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(model_name,model_file_path):\n",
    "    if model_name!='mistral':\n",
    "        model_load_string=\"load_\"+model_name\n",
    "        model_load_function=eval(model_load_string)\n",
    "        pipeline = model_load_function(model_file_path)\n",
    "    \n",
    "        model_inference_string=model_name+\"_inference_model\"\n",
    "        model_inference_function=eval(model_inference_string)\n",
    "    else:\n",
    "        mistral_model, mistral_tokenizer = load_mistral(model_file_path)\n",
    "        model_inference_function=eval(\"mistral7b_inference_model\")\n",
    "         \n",
    "    data_sample_types=['intersentence','intrasentence']\n",
    "    \n",
    "    for data_sample_type in data_sample_types:\n",
    "        if data_sample_type=='intersentence':\n",
    "            df=inter_df\n",
    "        else:\n",
    "            df=intra_df\n",
    "        model_outputs=[]\n",
    "        for idx, row in df.iterrows():\n",
    "            out=get_output(row, data_sample_type)\n",
    "            model_outputs.append(out)\n",
    "        df[model_name+'_output']=model_outputs\n",
    "        df.to_csv(model_name+'_'+data_sample_type+'_Output.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704fd1b8-21ec-4da9-b30c-ba49a52487dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample usage\n",
    "generate_output('tinyllama','/opt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
